{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfB8zKal2KVg"
   },
   "source": [
    "# Koelectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAHZV80l2E5s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers.activations import get_activation\n",
    "from transformers import (\n",
    "  ElectraPreTrainedModel,\n",
    "  ElectraModel,\n",
    "  ElectraConfig,\n",
    "  ElectraTokenizer,\n",
    "  BertConfig,\n",
    "  BertTokenizer\n",
    ")\n",
    "\n",
    "# MODEL_CLASSES = {\n",
    "#     \"koelectra-base\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
    "#     \"koelectra-small\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
    "#     \"koelectra-base-v2\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
    "#     \"koelectra-small-v2\": (ElectraConfig, koElectraForSequenceClassification, ElectraTokenizer),\n",
    "# }\n",
    "\n",
    "\n",
    "# def load_tokenizer(args):\n",
    "#   return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)\n",
    "\n",
    "\n",
    "class ElectraClassificationHead(nn.Module):\n",
    "  \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "  def __init__(self, config, num_labels):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(config.hidden_size, 4*config.hidden_size)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    self.out_proj = nn.Linear(4*config.hidden_size,num_labels)\n",
    "\n",
    "  def forward(self, features, **kwargs):\n",
    "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense(x)\n",
    "    x = get_activation(\"gelu\")(x)  # although BERT uses tanh here, it seems Electra authors used gelu here\n",
    "    x = self.dropout(x)\n",
    "    x = self.out_proj(x)\n",
    "    return x\n",
    "\n",
    "class koElectraForSequenceClassification(ElectraPreTrainedModel):\n",
    "  def __init__(self,\n",
    "               config,\n",
    "               num_labels):\n",
    "    super().__init__(config)\n",
    "    self.num_labels = num_labels\n",
    "    self.electra = ElectraModel(config)\n",
    "    self.classifier = ElectraClassificationHead(config, num_labels)\n",
    "    self.init_weights()\n",
    "\n",
    "  def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          position_ids=None,\n",
    "          head_mask=None,\n",
    "          inputs_embeds=None,\n",
    "          labels=None,\n",
    "          output_attentions=None,\n",
    "          output_hidden_states=None,\n",
    "  ):\n",
    "    r\"\"\"\n",
    "    labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "        Labels for computing the sequence classification/regression loss.\n",
    "        Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "        If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "        If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "    \"\"\"\n",
    "    discriminator_hidden_states = self.electra(\n",
    "      input_ids,\n",
    "      attention_mask,\n",
    "      token_type_ids,\n",
    "      position_ids,\n",
    "      head_mask,\n",
    "      inputs_embeds,\n",
    "      output_attentions,\n",
    "      output_hidden_states,\n",
    "    )\n",
    "\n",
    "    sequence_output = discriminator_hidden_states[0]\n",
    "    logits = self.classifier(sequence_output)\n",
    "\n",
    "    outputs = (logits,) + discriminator_hidden_states[1:]  # add hidden states and attention if they are here\n",
    "\n",
    "    if labels is not None:\n",
    "      if self.num_labels == 1:\n",
    "        #  We are doing regression\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "      else:\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "      outputs = (loss,) + outputs\n",
    "\n",
    "    return outputs  # (loss), (logits), (hidden_states), (attentions)\n",
    "\n",
    "def koelectra_input(tokenizer, str, device = None, max_seq_len = 512):\n",
    "  index_of_words = tokenizer.encode(str)\n",
    "  # token_type_ids = [0] * len(index_of_words)\n",
    "  attention_mask = [1] * len(index_of_words)\n",
    "\n",
    "  # Padding Length\n",
    "  padding_length = max_seq_len - len(index_of_words)\n",
    "\n",
    "  # Zero Padding\n",
    "  index_of_words += [0] * padding_length\n",
    "  # token_type_ids += [0] * padding_length\n",
    "  attention_mask += [0] * padding_length\n",
    "\n",
    "  data = {\n",
    "    'input_ids': torch.tensor([index_of_words]).to(device),\n",
    "    'attention_mask': torch.tensor([attention_mask]).to(device),\n",
    "  }\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1764,
     "status": "ok",
     "timestamp": 1743306831526,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "-HztHJH_6qhP",
    "outputId": "2ce2953a-8c92-44e0-9e1a-83254458e2eb"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"monologg/koelectra-base-discriminator\"\n",
    "electra_config = ElectraConfig.from_pretrained(model_name_or_path)\n",
    "model = koElectraForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path,\n",
    "                                                            config=electra_config,\n",
    "                                                            num_labels=359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1743306833950,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "4nku2rT-6xLY",
    "outputId": "a55e7b22-9d61-43ac-eea2-e4a907be875d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(32200, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=3072, out_features=359, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining\n",
    "\n",
    "from torchsummary import summary\n",
    "from pytorch_model_summary import summary as pt_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1743307847568,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "h9iY1UNOPlla",
    "outputId": "54f56220-5f45-4f81-a30e-49c066f010ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 110,771,713\n"
     ]
    }
   ],
   "source": [
    "# Get the model from Hugging Face(discriminator)\n",
    "model_name = \"monologg/koelectra-base-discriminator\"\n",
    "model99 = ElectraForPreTraining.from_pretrained(model_name)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "def count_parameters(model99):\n",
    "    return sum(p.numel() for p in model99.parameters())\n",
    "\n",
    "# Number of parameters output\n",
    "print(f\"Number of parameters: {count_parameters(model99):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "92ded3e5dd344394a76f11c9bfdf0569",
      "55c026ca96704768a2f6ac67c6d49e73",
      "8105174096cd49e899a649f521f831bf",
      "dc76ab1762d14a9ab8f37e80bee5d842",
      "dbdcad95387c4c9493554a4a058c10b6",
      "3d4e0992ff7c4a42b130e7ddee5eef80",
      "93424cfccf05461e9acae7e91a7073b9",
      "21fe458affb94b76955b6a86d2e84045",
      "b90bac47fd814beeb0401f56f56d19e2",
      "9f48438a862c428daf81b36428142440",
      "556a5609f74f49b696fa83e04b6421ca",
      "72abf68d32304e7e801e3d6687328241",
      "dce1d77b58794876ac502ecaafb932dd",
      "d28403558d9f4086a54a56ab223541bb",
      "3246b47e256c48c9a2934a8f0fe60334",
      "2e092284c88449429b5c262366131c05",
      "aecde98583fd4a079114efb48d4feb19",
      "fbad3fa960864d3b898630ba561f279f",
      "9bf9a447644544039bbb3abd8009c0d3",
      "8e228cb4918f4d689362c8d9220b8c57",
      "ea39020be41e44ad86c3176980cf98c0",
      "0919fa9799584530a71b93fe4cc0fbf1"
     ]
    },
    "executionInfo": {
     "elapsed": 3969,
     "status": "ok",
     "timestamp": 1743307866716,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "2ltGjP9OPuHI",
    "outputId": "edd85953-b722-49a9-c546-902c2f890a86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ded3e5dd344394a76f11c9bfdf0569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/463 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72abf68d32304e7e801e3d6687328241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/140M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForPreTraining were not initialized from the model checkpoint at monologg/koelectra-base-generator and are newly initialized: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 34,865,921\n"
     ]
    }
   ],
   "source": [
    "# Get the model from Hugging Face(generator)\n",
    "model_name = \"monologg/koelectra-base-generator\"\n",
    "model99 = ElectraForPreTraining.from_pretrained(model_name)\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "def count_parameters(model99):\n",
    "    return sum(p.numel() for p in model99.parameters())\n",
    "\n",
    "# Number of parameters output\n",
    "print(f\"Number of parameters: {count_parameters(model99):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1743306730067,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "6NbnLsLILZZQ",
    "outputId": "37e924b3-8537-4301-c990-4579fc1a6259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra.embeddings.word_embeddings.weight torch.Size([32200, 768])\n",
      "electra.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "electra.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "electra.embeddings.LayerNorm.weight torch.Size([768])\n",
      "electra.embeddings.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "electra.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "electra.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "electra.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "electra.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "electra.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "electra.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "electra.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "electra.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "electra.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "classifier.dense.weight torch.Size([3072, 768])\n",
      "classifier.dense.bias torch.Size([3072])\n",
      "classifier.out_proj.weight torch.Size([359, 3072])\n",
      "classifier.out_proj.bias torch.Size([359])\n"
     ]
    }
   ],
   "source": [
    "# Look into the model structure\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1959,
     "status": "ok",
     "timestamp": 1743305381815,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "JrNA_Wu9FvzA",
    "outputId": "2c1e25ee-72ab-49b5-82ad-314553d2a51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                                                       Parent Layers          Layer (type)           Input Shape         Param #     Tr. Param #\n",
      "================================================================================================================================================================================================\n",
      "                                                   koElectraForSequenceClassification/ElectraModel/ElectraEmbeddings           Embedding-1              [1, 512]      24,729,600      24,729,600\n",
      "                                                   koElectraForSequenceClassification/ElectraModel/ElectraEmbeddings           Embedding-2              [1, 512]           1,536           1,536\n",
      "                                                   koElectraForSequenceClassification/ElectraModel/ElectraEmbeddings           Embedding-3              [1, 512]         393,216         393,216\n",
      "                                                   koElectraForSequenceClassification/ElectraModel/ElectraEmbeddings           LayerNorm-4         [1, 512, 768]           1,536           1,536\n",
      "                                                   koElectraForSequenceClassification/ElectraModel/ElectraEmbeddings             Dropout-5         [1, 512, 768]               0               0\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention              Linear-6         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention              Linear-7         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention              Linear-8         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Dropout-9     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-10         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-11         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-12         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-13         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-14        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-15        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-16         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-17         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-18         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-19         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-20         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-21     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-22         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-23         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-24         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-25         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-26        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-27        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-28         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-29         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-30         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-31         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-32         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-33     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-34         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-35         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-36         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-37         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-38        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-39        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-40         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-41         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-42         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-43         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-44         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-45     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-46         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-47         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-48         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-49         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-50        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-51        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-52         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-53         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-54         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-55         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-56         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-57     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-58         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-59         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-60         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-61         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-62        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-63        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-64         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-65         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-66         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-67         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-68         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-69     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-70         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-71         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-72         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-73         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-74        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-75        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-76         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-77         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-78         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-79         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-80         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-81     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-82         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-83         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-84         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-85         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-86        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-87        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Dropout-88         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput          LayerNorm-89         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-90         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-91         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention             Linear-92         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Dropout-93     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput             Linear-94         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Dropout-95         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput          LayerNorm-96         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate             Linear-97         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate     GELUActivation-98        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput             Linear-99        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput           Dropout-100         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput         LayerNorm-101         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-102         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-103         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-104         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention           Dropout-105     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Linear-106         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput           Dropout-107         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput         LayerNorm-108         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate            Linear-109         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate    GELUActivation-110        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Linear-111        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput           Dropout-112         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput         LayerNorm-113         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-114         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-115         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-116         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention           Dropout-117     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Linear-118         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput           Dropout-119         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput         LayerNorm-120         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate            Linear-121         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate    GELUActivation-122        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Linear-123        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput           Dropout-124         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput         LayerNorm-125         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-126         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-127         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-128         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention           Dropout-129     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Linear-130         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput           Dropout-131         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput         LayerNorm-132         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate            Linear-133         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate    GELUActivation-134        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Linear-135        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput           Dropout-136         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput         LayerNorm-137         [1, 512, 768]           1,536           1,536\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-138         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-139         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention            Linear-140         [1, 512, 768]         590,592         590,592\n",
      "   koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfAttention           Dropout-141     [1, 12, 512, 512]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput            Linear-142         [1, 512, 768]         590,592         590,592\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput           Dropout-143         [1, 512, 768]               0               0\n",
      "      koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraAttention/ElectraSelfOutput         LayerNorm-144         [1, 512, 768]           1,536           1,536\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate            Linear-145         [1, 512, 768]       2,362,368       2,362,368\n",
      "                     koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraIntermediate    GELUActivation-146        [1, 512, 3072]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput            Linear-147        [1, 512, 3072]       2,360,064       2,360,064\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput           Dropout-148         [1, 512, 768]               0               0\n",
      "                           koElectraForSequenceClassification/ElectraModel/ElectraEncoder/ElectraLayer/ElectraOutput         LayerNorm-149         [1, 512, 768]           1,536           1,536\n",
      "                                                        koElectraForSequenceClassification/ElectraClassificationHead           Dropout-150              [1, 768]               0               0\n",
      "                                                        koElectraForSequenceClassification/ElectraClassificationHead            Linear-151              [1, 768]       2,362,368       2,362,368\n",
      "                                                        koElectraForSequenceClassification/ElectraClassificationHead            Linear-152             [1, 3072]       1,103,207       1,103,207\n",
      "================================================================================================================================================================================================\n",
      "Total params: 113,645,927\n",
      "Trainable params: 113,645,927\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Summarize model architecture\n",
    "dummy_input5 = torch.randint(0, 32200, (1, 512))  # (batch_size, sequence_length)\n",
    "print(pt_summary(model, dummy_input5, show_input=True, max_depth=None, show_parent_layers=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6555,
     "status": "ok",
     "timestamp": 1743299692557,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "yypKhC2RVWHq",
    "outputId": "03fe81ec-86ef-4c15-c5ab-3cb547cd826f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kogpt2_transformers\n",
      "  Downloading kogpt2_transformers-0.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from kogpt2_transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from kogpt2_transformers) (4.50.0)\n",
      "Requirement already satisfied: tokenizers>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from kogpt2_transformers) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.7.0->kogpt2_transformers) (0.29.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.1.0->kogpt2_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.1.0->kogpt2_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.0.0->kogpt2_transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.1.0->kogpt2_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.0.0->kogpt2_transformers) (2025.1.31)\n",
      "Downloading kogpt2_transformers-0.4.0-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: kogpt2_transformers\n",
      "Successfully installed kogpt2_transformers-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kogpt2_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBRYRx4l2PBj"
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "#KoGPT2 configuration\n",
    "# kogpt2_config = {\n",
    "#     \"initializer_range\": 0.02,\n",
    "#     \"layer_norm_epsilon\": 1e-05,\n",
    "#     \"n_ctx\": 1024,\n",
    "#     \"n_embd\": 768,\n",
    "#     \"n_head\": 12,\n",
    "#     \"n_layer\": 12,\n",
    "#     \"n_positions\": 1024,\n",
    "#     \"vocab_size\": 50000,\n",
    "#     \"activation_function\": \"gelu\"\n",
    "# }\n",
    "\n",
    "import torch.nn as nn\n",
    "from kogpt2_transformers import get_kogpt2_model\n",
    "\n",
    "\n",
    "class DialogKoGPT2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DialogKoGPT2, self).__init__()\n",
    "    self.kogpt2 = get_kogpt2_model()\n",
    "\n",
    "  def generate(self,\n",
    "               input_ids,\n",
    "               do_sample=True,\n",
    "               max_length= 30,\n",
    "               top_p=0.92,\n",
    "               top_k=50,\n",
    "               temperature= 0.6,\n",
    "               no_repeat_ngram_size =None,\n",
    "               num_return_sequences=1,\n",
    "               early_stopping=False,\n",
    "               ):\n",
    "    return self.kogpt2.generate(input_ids,\n",
    "               do_sample=do_sample,\n",
    "               max_length=max_length,\n",
    "               top_p = top_p,\n",
    "               top_k=top_k,\n",
    "               temperature=temperature,\n",
    "               no_repeat_ngram_size= no_repeat_ngram_size,\n",
    "               num_return_sequences=num_return_sequences,\n",
    "               early_stopping = early_stopping,\n",
    "              )\n",
    "\n",
    "  def forward(self, input, labels = None):\n",
    "    if labels is not None:\n",
    "      outputs = self.kogpt2(input, labels=labels)\n",
    "    else:\n",
    "      outputs = self.kogpt2(input)\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BldAkvsKUsWa"
   },
   "outputs": [],
   "source": [
    "model2 = DialogKoGPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1743209355452,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "GnLR21qEX8xf",
    "outputId": "37c4f89e-4ba2-4fb3-b2f4-febb13ad87fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialogKoGPT2(\n",
      "  (kogpt2): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50000, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model2)  # config코드 동작X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1743208762653,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "4CljRYetVsBA",
    "outputId": "b6f87828-800c-42de-8028-2b903d6fdade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialogKoGPT2(\n",
      "  (kogpt2): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50000, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model2)  # config코드 동작O  # 결과는 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5512,
     "status": "ok",
     "timestamp": 1743304367079,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "CYkIANNVCW5S",
    "outputId": "22cec994-99ac-4107-d5bd-391e82a0ed09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "            Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "             Embedding-1           [1, 1024]      38,400,000      38,400,000\n",
      "             Embedding-2           [1, 1024]         786,432         786,432\n",
      "               Dropout-3      [1, 1024, 768]               0               0\n",
      "             LayerNorm-4      [1, 1024, 768]           1,536           1,536\n",
      "                Conv1D-5      [1, 1024, 768]       1,771,776       1,771,776\n",
      "                Conv1D-6      [1, 1024, 768]         590,592         590,592\n",
      "               Dropout-7      [1, 1024, 768]               0               0\n",
      "             LayerNorm-8      [1, 1024, 768]           1,536           1,536\n",
      "                Conv1D-9      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-10     [1, 1024, 3072]               0               0\n",
      "               Conv1D-11     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-12      [1, 1024, 768]               0               0\n",
      "            LayerNorm-13      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-14      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-15      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-16      [1, 1024, 768]               0               0\n",
      "            LayerNorm-17      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-18      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-19     [1, 1024, 3072]               0               0\n",
      "               Conv1D-20     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-21      [1, 1024, 768]               0               0\n",
      "            LayerNorm-22      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-23      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-24      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-25      [1, 1024, 768]               0               0\n",
      "            LayerNorm-26      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-27      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-28     [1, 1024, 3072]               0               0\n",
      "               Conv1D-29     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-30      [1, 1024, 768]               0               0\n",
      "            LayerNorm-31      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-32      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-33      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-34      [1, 1024, 768]               0               0\n",
      "            LayerNorm-35      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-36      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-37     [1, 1024, 3072]               0               0\n",
      "               Conv1D-38     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-39      [1, 1024, 768]               0               0\n",
      "            LayerNorm-40      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-41      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-42      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-43      [1, 1024, 768]               0               0\n",
      "            LayerNorm-44      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-45      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-46     [1, 1024, 3072]               0               0\n",
      "               Conv1D-47     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-48      [1, 1024, 768]               0               0\n",
      "            LayerNorm-49      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-50      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-51      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-52      [1, 1024, 768]               0               0\n",
      "            LayerNorm-53      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-54      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-55     [1, 1024, 3072]               0               0\n",
      "               Conv1D-56     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-57      [1, 1024, 768]               0               0\n",
      "            LayerNorm-58      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-59      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-60      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-61      [1, 1024, 768]               0               0\n",
      "            LayerNorm-62      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-63      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-64     [1, 1024, 3072]               0               0\n",
      "               Conv1D-65     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-66      [1, 1024, 768]               0               0\n",
      "            LayerNorm-67      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-68      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-69      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-70      [1, 1024, 768]               0               0\n",
      "            LayerNorm-71      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-72      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-73     [1, 1024, 3072]               0               0\n",
      "               Conv1D-74     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-75      [1, 1024, 768]               0               0\n",
      "            LayerNorm-76      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-77      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-78      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-79      [1, 1024, 768]               0               0\n",
      "            LayerNorm-80      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-81      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-82     [1, 1024, 3072]               0               0\n",
      "               Conv1D-83     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-84      [1, 1024, 768]               0               0\n",
      "            LayerNorm-85      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-86      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-87      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-88      [1, 1024, 768]               0               0\n",
      "            LayerNorm-89      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-90      [1, 1024, 768]       2,362,368       2,362,368\n",
      "    NewGELUActivation-91     [1, 1024, 3072]               0               0\n",
      "               Conv1D-92     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "              Dropout-93      [1, 1024, 768]               0               0\n",
      "            LayerNorm-94      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-95      [1, 1024, 768]       1,771,776       1,771,776\n",
      "               Conv1D-96      [1, 1024, 768]         590,592         590,592\n",
      "              Dropout-97      [1, 1024, 768]               0               0\n",
      "            LayerNorm-98      [1, 1024, 768]           1,536           1,536\n",
      "               Conv1D-99      [1, 1024, 768]       2,362,368       2,362,368\n",
      "   NewGELUActivation-100     [1, 1024, 3072]               0               0\n",
      "              Conv1D-101     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "             Dropout-102      [1, 1024, 768]               0               0\n",
      "           LayerNorm-103      [1, 1024, 768]           1,536           1,536\n",
      "              Conv1D-104      [1, 1024, 768]       1,771,776       1,771,776\n",
      "              Conv1D-105      [1, 1024, 768]         590,592         590,592\n",
      "             Dropout-106      [1, 1024, 768]               0               0\n",
      "           LayerNorm-107      [1, 1024, 768]           1,536           1,536\n",
      "              Conv1D-108      [1, 1024, 768]       2,362,368       2,362,368\n",
      "   NewGELUActivation-109     [1, 1024, 3072]               0               0\n",
      "              Conv1D-110     [1, 1024, 3072]       2,360,064       2,360,064\n",
      "             Dropout-111      [1, 1024, 768]               0               0\n",
      "           LayerNorm-112      [1, 1024, 768]           1,536           1,536\n",
      "              Linear-113      [1, 1024, 768]      38,400,000      38,400,000\n",
      "=============================================================================\n",
      "Total params: 162,642,432\n",
      "Trainable params: 162,642,432\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(pt_summary(model2, dummy_input, show_input=True, max_depth=None, show_parent_layers=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6491,
     "status": "ok",
     "timestamp": 1743303893391,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "Y48-DUuQ_AfT",
    "outputId": "e39608da-1082-4fc6-a0c5-32e639e38ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "        Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "   GPT2LMHeadModel-1           [1, 1024]     124,242,432     124,242,432\n",
      "=========================================================================\n",
      "Total params: 124,242,432\n",
      "Trainable params: 124,242,432\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model2 = DialogKoGPT2()\n",
    "\n",
    "dummy_input = torch.randint(0, 50000, (1, 1024))  # (batch_size, sequence_length)\n",
    "\n",
    "print(pt_summary(model2, dummy_input, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13257,
     "status": "ok",
     "timestamp": 1743306564287,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "hjYG_h-t_oZu",
    "outputId": "fbf60e5e-4198-48ad-fc6a-29399460876e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "        Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "   GPT2LMHeadModel-1           [2, 1024]     124,242,432     124,242,432\n",
      "=========================================================================\n",
      "Total params: 124,242,432\n",
      "Trainable params: 124,242,432\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model3 = DialogKoGPT2()\n",
    "\n",
    "dummy_input3 = torch.randint(0, 50000, (2, 1024))  # (batch_size, sequence_length)\n",
    "\n",
    "print(pt_summary(model3, dummy_input3, show_input=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10393,
     "status": "ok",
     "timestamp": 1743304014059,
     "user": {
      "displayName": "Seulbi Ryu",
      "userId": "10359868065208831031"
     },
     "user_tz": -540
    },
    "id": "4Wy7XJu6A5TQ",
    "outputId": "2e0b3ff5-6cf4-4f1b-e438-d51f69a0aa03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                    Parent Layers             Layer (type)         Input Shape         Param #     Tr. Param #\n",
      "==============================================================================================================================================\n",
      "                           DialogKoGPT2/GPT2LMHeadModel/GPT2Model              Embedding-1           [2, 1024]      38,400,000      38,400,000\n",
      "                           DialogKoGPT2/GPT2LMHeadModel/GPT2Model              Embedding-2           [1, 1024]         786,432         786,432\n",
      "                           DialogKoGPT2/GPT2LMHeadModel/GPT2Model                Dropout-3      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block              LayerNorm-4      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                 Conv1D-5      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                 Conv1D-6      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Dropout-7      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block              LayerNorm-8      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                 Conv1D-9      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-10     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-11     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-12      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-13      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-14      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-15      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-16      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-17      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-18      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-19     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-20     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-21      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-22      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-23      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-24      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-25      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-26      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-27      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-28     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-29     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-30      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-31      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-32      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-33      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-34      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-35      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-36      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-37     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-38     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-39      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-40      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-41      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-42      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-43      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-44      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-45      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-46     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-47     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-48      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-49      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-50      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-51      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-52      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-53      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-54      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-55     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-56     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-57      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-58      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-59      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-60      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-61      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-62      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-63      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-64     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-65     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-66      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-67      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-68      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-69      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-70      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-71      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-72      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-73     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-74     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-75      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-76      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-77      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-78      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-79      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-80      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-81      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-82     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-83     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-84      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-85      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-86      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-87      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-88      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-89      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-90      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP     NewGELUActivation-91     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-92     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Dropout-93      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-94      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-95      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention                Conv1D-96      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Dropout-97      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block             LayerNorm-98      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP                Conv1D-99      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP    NewGELUActivation-100     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Conv1D-101     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP              Dropout-102      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block            LayerNorm-103      [2, 1024, 768]           1,536           1,536\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Conv1D-104      [2, 1024, 768]       1,771,776       1,771,776\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention               Conv1D-105      [2, 1024, 768]         590,592         590,592\n",
      "   DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2Attention              Dropout-106      [2, 1024, 768]               0               0\n",
      "                 DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block            LayerNorm-107      [2, 1024, 768]           1,536           1,536\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Conv1D-108      [2, 1024, 768]       2,362,368       2,362,368\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP    NewGELUActivation-109     [2, 1024, 3072]               0               0\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP               Conv1D-110     [2, 1024, 3072]       2,360,064       2,360,064\n",
      "         DialogKoGPT2/GPT2LMHeadModel/GPT2Model/GPT2Block/GPT2MLP              Dropout-111      [2, 1024, 768]               0               0\n",
      "                           DialogKoGPT2/GPT2LMHeadModel/GPT2Model            LayerNorm-112      [2, 1024, 768]           1,536           1,536\n",
      "                                     DialogKoGPT2/GPT2LMHeadModel               Linear-113      [2, 1024, 768]      38,400,000      38,400,000\n",
      "==============================================================================================================================================\n",
      "Total params: 162,642,432\n",
      "Trainable params: 162,642,432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(pt_summary(model3, dummy_input3, show_input=True, max_depth=None, show_parent_layers=True))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN7yStFuVJ/tr+U6/HVIooW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0919fa9799584530a71b93fe4cc0fbf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21fe458affb94b76955b6a86d2e84045": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e092284c88449429b5c262366131c05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3246b47e256c48c9a2934a8f0fe60334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea39020be41e44ad86c3176980cf98c0",
      "placeholder": "​",
      "style": "IPY_MODEL_0919fa9799584530a71b93fe4cc0fbf1",
      "value": " 140M/140M [00:02&lt;00:00, 80.5MB/s]"
     }
    },
    "3d4e0992ff7c4a42b130e7ddee5eef80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "556a5609f74f49b696fa83e04b6421ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55c026ca96704768a2f6ac67c6d49e73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d4e0992ff7c4a42b130e7ddee5eef80",
      "placeholder": "​",
      "style": "IPY_MODEL_93424cfccf05461e9acae7e91a7073b9",
      "value": "config.json: 100%"
     }
    },
    "72abf68d32304e7e801e3d6687328241": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dce1d77b58794876ac502ecaafb932dd",
       "IPY_MODEL_d28403558d9f4086a54a56ab223541bb",
       "IPY_MODEL_3246b47e256c48c9a2934a8f0fe60334"
      ],
      "layout": "IPY_MODEL_2e092284c88449429b5c262366131c05"
     }
    },
    "8105174096cd49e899a649f521f831bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21fe458affb94b76955b6a86d2e84045",
      "max": 463,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b90bac47fd814beeb0401f56f56d19e2",
      "value": 463
     }
    },
    "8e228cb4918f4d689362c8d9220b8c57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92ded3e5dd344394a76f11c9bfdf0569": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55c026ca96704768a2f6ac67c6d49e73",
       "IPY_MODEL_8105174096cd49e899a649f521f831bf",
       "IPY_MODEL_dc76ab1762d14a9ab8f37e80bee5d842"
      ],
      "layout": "IPY_MODEL_dbdcad95387c4c9493554a4a058c10b6"
     }
    },
    "93424cfccf05461e9acae7e91a7073b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bf9a447644544039bbb3abd8009c0d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f48438a862c428daf81b36428142440": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aecde98583fd4a079114efb48d4feb19": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b90bac47fd814beeb0401f56f56d19e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d28403558d9f4086a54a56ab223541bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bf9a447644544039bbb3abd8009c0d3",
      "max": 140173349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e228cb4918f4d689362c8d9220b8c57",
      "value": 140173349
     }
    },
    "dbdcad95387c4c9493554a4a058c10b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc76ab1762d14a9ab8f37e80bee5d842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f48438a862c428daf81b36428142440",
      "placeholder": "​",
      "style": "IPY_MODEL_556a5609f74f49b696fa83e04b6421ca",
      "value": " 463/463 [00:00&lt;00:00, 24.4kB/s]"
     }
    },
    "dce1d77b58794876ac502ecaafb932dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aecde98583fd4a079114efb48d4feb19",
      "placeholder": "​",
      "style": "IPY_MODEL_fbad3fa960864d3b898630ba561f279f",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "ea39020be41e44ad86c3176980cf98c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fbad3fa960864d3b898630ba561f279f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
